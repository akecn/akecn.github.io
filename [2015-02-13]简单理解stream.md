## 简单理解Stream

### 概念理解

首先关于Stream的一些基本信息就不在说了。下文主要描述个人对Stream使用的一些理解。也许有错误和不妥的地方，请见谅。

Stream有许多的类型，包括Readable、Writeable、Duplex、Transform和Classic等。但本质上其实只有只读、只写和双工三种。

其中Transform和Duplex就是双工流。

数据的流转方式就是，从可读流中读取数据，然后写入到可写流。

那么Transform和Duplex都是双工类型的，它们之间的差别是什么呢？

从文档说明中，我们大致也能看出差别。Transform Stream需要实现`_transform()`方法，而 Duplex Stream则需要实现`_read()`和`_write()`方法。

```
                             Duplex Stream
                          ------------------|
                    Read  <-----               External Source
            You           ------------------|   
                    Write ----->               External Sink
                          ------------------|
            You don't get what you write. It is sent to another source.
```

```
                                 Transform Stream
                           --------------|--------------
            You     Write  ---->                   ---->  Read  You
                           --------------|--------------
            You write something, it is transformed, then you read something.
```

如上两幅字符图可能会更直观些。[via. strackoverflow](http://stackoverflow.com/questions/18335499/nodejs-whats-the-difference-between-a-duplex-stream-and-a-transform-stream)

transform意味着，stream读取数据后通过特定的处理，最后将结果输出。而Duplex则分别实现读和写操作。

transform就像是流转数据的中间管道，数据从一端进来又从另一端出去，而duplex则可以是管道的两端（只读和只写流也属于两端吧）。

上面是理论的理解，下面我们说说代码和应用。

### 代码和使用

在使用上，`through2`可能是我们使用最多的Stream模块之一。相比直接使用Stream对象来说，使用through2模块可以比较方便的实现自己的需求。所以下面的代码大多是基于through2模块的。

```
var through2 = require('through2');
var stream = through2();
stream.write('hello');
stream.write(' ');
stream.write('world!');
stream.pipe(process.stdout);
```

上面的代码最终会在命令行中输出`hello world!`。分解一下上面的代码：

首先我们引入`through2`模块，并实例化一个stream对象（Transform Stream）。然后设置流下游的操作——标准输出。最后我们分批次的将内容写入stream对象，很自然的内容就会流到`process.stdout`中进行处理。

这里包含几个注意点：

1. 当我们通过`write()`方法写入内容的时候，内容只会缓存到内存中，只有出现了下游接收的stream对象才会将内容往下流（想想也是，如果没有下游的流，内容又能流到哪里呢？）。
2. `process.stdout`和`process.stdin`也是流实例。
3. 虽然`through2`是Transform Stream，但不妨碍它作为流操作的起点（或终点）。

```
var through2 = require('through2'),
	fs = require('fs');
var stream = through2.obj(function(file, enc, callback) {
	// do something with file buffer
	callback(null, file);
	// or
	this.push(file);
	callback();
});
stream.pipe(through2.obj(function(file, enc, callback) {
	process.stdout(file.toString());
}));
var file = fs.readFileSync('/path/to/file');
stream.write(file);
```

第一个代码示例是在传输字符串内容。但实际上更多的时候，我们传输的是对象——通过调用`through2.obj()`创建一个用于传递对象的流实例。而创建实例时传入的函数参数则是用于处理每一段流数据的处理函数。其中第一个参数表示要处理的数据，第二个参数表示传递的数据的编码类型，最后一个则是回调函数——单数据处理完成以后，需要调用回调函数以通知程序进行下一个数据流的处理。

如果不调用callback会怎么样？不调用的话，stream就只会处理当前的数据流，并停止响应其他的数据，也就是说，它不会在读取上游传递下来的另一个数据。

数据流可以理解为一段一段的水流，上游源源不断的流下，下游不断的处理并继续向下游流动。

当我们调用write方法进行一次写操作就会产生一段数据流。每一段数据流会单独的经过处理函数。如果我们有很多的流处理逻辑，每一段数据流相互之间不会影响，会分别流动直到终点。

所以对于第一个例子，如果我们添加一个中间流处理，并且不去调用callback，我们看到的就只有`hello`这个结果。

而第二个例子只调用了一次write，所以暂时只会产生一个数据流，调用一次流处理函数。

第二个例子基本上就是`gulp.src`的基本实现原理了，每一个文件是一段数据流。当然实际上会复杂很多。比如传递的数据并不是原生的文件对象，而是`Vinyl`对象，通过`glob-stream`模块进行文件查询处理等等。

按上文提到的，对于每一个数据流都会单独被处理并流到下游。那如果我们需要对文件进行合并怎么办？这里就涉及到Transform Stream的flush逻辑了。

在实现一个`Transform Stream`流的时候，我们除了要实现`_transform`方法，还可以实现一个`_flush`方法，它会在所有的流数据都处理完成以后被调用。那么怎么判断数据完结呢？上面我们更多的是在说流的源源不断的特性，但如果需要结束一个stream的数据传递，可以通过调用可写流的`end()`方法结束数据传递，它的另外一个调用方式是`stream.write(null)`。对于HTTP Response对象大家应该不陌生，它也是一个流对象，所以我们还可以套用`res.end(content)`的调用方式。

```
var through2 = require('through2');
var list = [];
var stream = through2(function(content, enc, cb) {
	this.push(content);
	cb();
}, function(cb) {
	console.log(Buffer.concat(list).toString()); // ab
	cb();
});))
var transform = stream._transform;
stream._transform = function(content, enc, callback) {
	list.push(content);
	transform.call(this, content, enc, callback);
}
stream.write('a');
stream.write('b');
stream.end();
stream.pipe(process.stdout)
```

上面的代码，在调用了`end()`方法以后，就会输出所有的数据字段拼接的内容。实现方法就是通过重写`_transform()`方法，将数据收集起来。最后在执行flush的时候将数据拼接并输出。

我们还可以通过`data`事件，避免重写这个看起来很可怕的方式来收集数据。但文档中提到，如果使用了`data`事件，则stream内部的处理方式会按老的逻辑来处理流数据。不管其他方面，单从发展来讲，还是使用Stream2 API看起来会高端点不是么 ；）

因为这个demo是通过through2模块来实现的，所以这里要注意一点：虽然我们将数据拼接到了一起，但如果我们期望的是对合并的数据进行后续的处理的话，我们就不能直接在stream上继续pipe了，否则后续操作处理的数据还是2个数据流（流处理逻辑执行的次数是2次）。所以合理的做法应该是将后续的操作都放到flush回调中进行流转。

如果我们期望更多定制性的东西，自己实现一个流对象。只需要继承合适的Stream对象，并实现Stream对应需要实现的api即可。比如[buffer list](http://npmjs.com/package/bl)，这个组件做的也是将buffer内容进行合并，但它是通过实现一个Duplex流对象来达到目的的。


除了上面比较常用的一些内容，流对象还有许多的api，这里就不再说了，欢迎大家讨论~~
